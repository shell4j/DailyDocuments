#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass paper
\begin_preamble
% 如果没有这一句命令，XeTeX会出错，原因参见
% http://bbs.ctex.org/viewthread.php?tid=60547
\DeclareRobustCommand\nobreakspace{\leavevmode\nobreak\ }
\usepackage{xeCJK}
\setCJKmainfont{WenQuanYi Zen Hei} 
\XeTeXlinebreaklocale "zh"
\XeTeXlinebreakskip = 0pt plus 1pt
\end_preamble
\options UTF8,adobefonts
\use_default_options true
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language chinese-simplified
\language_package none
\inputencoding utf8-plain
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Spark学习指南
\end_layout

\begin_layout Author
黄勇
\end_layout

\begin_layout Section
spark配置和使用
\end_layout

\begin_layout Standard
使用spark和ipython方法：进入spark的主目录运行如下代码，从而可以替换spark内置的python，而且可以使用下载的python包。也可以使用s
park和RStudio运行sparkR，或者直接使用SparkR运行交互式代码，如果需要运行R/python的脚本，可以运行bin目录下的spark-subm
it。
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
IPYTHON=1 IPYTHON_OPTS="--pylab" ./bin/pyspark 
\end_layout

\begin_layout Plain Layout
#在Rstudio中运行sparkR
\end_layout

\begin_layout Plain Layout
Sys.setenv(SPARK_HOME = "/path/to/spark-1.6.0")
\end_layout

\begin_layout Plain Layout
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
 
\end_layout

\begin_layout Plain Layout
#下一句出问题，情况未知
\end_layout

\begin_layout Plain Layout
sc <- sparkR.init(master = "local[2]", sparkEnvir = list(spark.driver.memory="2g"))
\end_layout

\end_inset


\end_layout

\begin_layout Section
Spark Core简介
\end_layout

\begin_layout Standard
从高层次来说，每个spark应用由一个运行着用户的main方法的驱动程序和在集群上执行不同的操作组成，spark的主要抽象提供了RDD，是一系列的集群上的被切分
的、能并行计算的节点。RDDs通过启动HDFS（或者其他支持HDFS的文件系统）被创建、或者通过创建一个存在的driver program和transformi
ng的Scala集合（RDDs are created by starting with a file in the Hadoop file system
 (or any other Hadoop-supported file system), or an existing Scala collection
 in the driver program, and transforming it）。用户也能使用spark来持久化内存中的RDD，允许spark通过并行操
作重用。RDDs能自动从节点失败中恢复。
\end_layout

\begin_layout Standard
第二个抽象是spark在并行操作中共享变量能被使用。
\end_layout

\begin_layout Section
Spark Streaming 简介
\end_layout

\begin_layout Section
GraphX属性图模型
\end_layout

\begin_layout Standard
Spark GraphX基于属性图模型构建图算法。属性图指图的属性和边都可以设置属性，此外，GraphX除了继承一些基础的图操作（如子图、节点合并、信息聚集等）
、实现Pregal API 等算法，也包括一系列的正在增长的图算法集合，从而可以简化图分析任务。
\end_layout

\begin_layout Standard
GraphX继承了Spark的RDD（全称为Resilient Distributed Datasets，是一个容错的、并行的数据结构，可以让用户显式地将数据存
储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了m
onad模式，很好地契合了Scala的集合操作），并建立了高层次的抽象模型。
\end_layout

\begin_layout Section
GraphX基本图操作
\end_layout

\begin_layout Section
SparkR和DataFrames
\end_layout

\begin_layout Standard
SparkR是一个使用轻量级的前端来连接Apache SparkR的语言包。SparkR提供了分布式的数据框来支持selection、filtering、agg
regation等操作（和R语言的dataframe、dplyr包类似，但支持更大规模的数据操作），SparkR也支持分布式的机器学习。
\end_layout

\begin_layout Subsection
SparkR数据框
\end_layout

\begin_layout Standard
SparkR的DataFrames被组织成一个分布式的数据集合，在概念上等于关系数据库的一个表或者R语言中的一个dataframe。但是SparkR有更高级的、
丰富的优化。Dataframes能从磁盘阵列源进行构建：结构化的数据文件、Hive中的表、外部的数据库、R本地的数据框。
\end_layout

\begin_layout Subsubsection*
使用SparkContext和SQLContext
\end_layout

\begin_layout Standard
SparkR使用SparkContext将R语言连接到Spark Cluster上，你可以通过使用SparkR.init创建SparkContext，并进入选项如
应用名称、依赖的Spark包等。为了能使用dataframes，我们需要使用能从SparkContext中创建的SQLContext。
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
sc<-sparkR.init()
\end_layout

\begin_layout Plain Layout
sqlcontext<-sparkRSQL.init(sc)
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
使用RStudio
\end_layout

\begin_layout Subsubsection*
创建使用数据框
\end_layout

\begin_layout Standard
应用程序能够利用SQLContext,可以从本地的R数据框、Hive的表或者其他的数据源创建数据框。对于数据框的使用可以是selecting行和列、分组和聚集、
对某一列操作、或者运行SQL查询语言。
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
选择和过滤
\end_layout

\begin_layout Plain Layout
head(select(iris,
\begin_inset Quotes erd
\end_inset

Species
\begin_inset Quotes erd
\end_inset

))
\end_layout

\begin_layout Plain Layout
head(filter(df,df$waiting<50))
\end_layout

\begin_layout Plain Layout
分组和聚集
\end_layout

\begin_layout Plain Layout
head(summarize(groupBy(df,df$waiting),count=n(df$waiting))
\end_layout

\begin_layout Plain Layout
waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting))
\end_layout

\begin_layout Plain Layout
执行查询和SQL语句
\end_layout

\begin_layout Plain Layout
people <- read.df(sqlContext, "./examples/src/main/resources/people.json",
 "json")
\end_layout

\begin_layout Plain Layout
# #Register this DataFrame as a table.
 registerTempTable(people, "people")
\end_layout

\begin_layout Plain Layout
teenagers <- sql(sqlContext, "SELECT name FROM people WHERE age >= 13 AND
 age <= 19")
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
SparkR机器学习模型
\end_layout

\begin_layout Standard
略，同正常的R统计和数据分析方法。
\end_layout

\begin_layout Subsection
R命名函数冲突
\end_layout

\begin_layout Standard
注意当使用sparkR和Rstudio时，有些R包中的函数会引起命名冲突，谨慎使用。
\end_layout

\end_body
\end_document
